{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8268323-ce48-43ed-95d4-2fad21c70065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.google.com/document/d/1lrYEnBW5aqioXriEt47KI_o3JUsCK9fShqySyNbKUn0/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6f52c3-028b-461c-89eb-a89fb5494806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint as pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "951892e7-15b3-4cc1-8eab-3900b9f66ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27455, 785)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"sign_mnist_train.csv\")\n",
    "y_train = train_data['label'].to_numpy()\n",
    "x_train = train_data.drop(['label'], axis = 1).to_numpy()\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5363c11-9aa6-4faa-87e5-cbecb7be5945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7172, 785)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"sign_mnist_test.csv\")\n",
    "y_test = test_data['label'].to_numpy()\n",
    "x_test = test_data.drop(['label'], axis = 1).to_numpy()\n",
    "\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4ca99dc-0161-40a6-96d1-22e5e0021ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  6,  2, 13, 16,  8, 22, 18, 10, 20, 17, 19, 21, 23, 24,  1, 12,\n",
       "       11, 15,  4,  0,  5,  7, 14], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = train_data['label'].unique()\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eccb27b-7d47-44fc-a060-103342dd1f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(classes)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0421c95e-a4ab-4650-a990-7bd8bfe0fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([np.reshape(i, (28,28)) for i in x_train])\n",
    "x_test = np.array([np.reshape(i, (28,28)) for i in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "791b75c8-16b1-44bb-974c-0d912ba1ba24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27455, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c0a35ef-f094-4542-8624-b17ee9221728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.array(x_train.iloc[:,:])\n",
    "x_train = np.array([np.reshape(i, (28,28)) for i in x_train])\n",
    "# x_test = np.array(x_test.iloc[:,:])\n",
    "x_test = np.array([np.reshape(i, (28,28)) for i in x_test])\n",
    "num_classes = 26\n",
    "y_train = np.array(y_train).reshape(-1)\n",
    "y_test = np.array(y_test).reshape(-1)\n",
    "y_train = np.eye(num_classes)[y_train]\n",
    "y_test = np.eye(num_classes)[y_test]\n",
    "x_train = x_train.reshape((27455, 28, 28, 1))\n",
    "x_test = x_test.reshape((7172, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a1c9fe0-d39e-4438-84b9-8304c8c0ca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ce630f6-bcd8-4be1-997a-83e6820a439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = keras.Sequential()\n",
    "classifier.add(layers.Conv2D(filters=8, kernel_size=(3,3),strides=(1,1),padding='same',input_shape=(28,28,1),activation='relu', data_format='channels_last'))\n",
    "classifier.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(layers.Conv2D(filters=16, kernel_size=(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "classifier.add(layers.Dropout(0.5))\n",
    "classifier.add(layers.MaxPooling2D(pool_size=(4,4)))\n",
    "classifier.add(layers.Dense(128, activation='relu'))\n",
    "classifier.add(layers.Flatten())\n",
    "classifier.add(layers.Dense(26, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "695b7a4f-3ef2-47e8-87ff-7726eeb0d508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "275/275 [==============================] - 28s 86ms/step - loss: 3.5898 - accuracy: 0.0461\n",
      "Epoch 2/50\n",
      "275/275 [==============================] - 15s 55ms/step - loss: 3.1684 - accuracy: 0.0644\n",
      "Epoch 3/50\n",
      "275/275 [==============================] - 14s 50ms/step - loss: 3.0139 - accuracy: 0.1103\n",
      "Epoch 4/50\n",
      "275/275 [==============================] - 14s 51ms/step - loss: 2.1941 - accuracy: 0.3413\n",
      "Epoch 5/50\n",
      "275/275 [==============================] - 13s 46ms/step - loss: 1.2776 - accuracy: 0.5993\n",
      "Epoch 6/50\n",
      "275/275 [==============================] - 13s 45ms/step - loss: 0.8933 - accuracy: 0.7155\n",
      "Epoch 7/50\n",
      "275/275 [==============================] - 14s 50ms/step - loss: 0.7253 - accuracy: 0.7660\n",
      "Epoch 8/50\n",
      "275/275 [==============================] - 14s 52ms/step - loss: 0.6239 - accuracy: 0.7908\n",
      "Epoch 9/50\n",
      "275/275 [==============================] - 14s 51ms/step - loss: 0.5568 - accuracy: 0.8152\n",
      "Epoch 10/50\n",
      "275/275 [==============================] - 14s 49ms/step - loss: 0.5305 - accuracy: 0.8244\n",
      "Epoch 11/50\n",
      "275/275 [==============================] - 16s 58ms/step - loss: 0.4775 - accuracy: 0.8413\n",
      "Epoch 12/50\n",
      "275/275 [==============================] - 14s 51ms/step - loss: 0.4273 - accuracy: 0.8565\n",
      "Epoch 13/50\n",
      "275/275 [==============================] - 13s 48ms/step - loss: 0.3910 - accuracy: 0.8696\n",
      "Epoch 14/50\n",
      "275/275 [==============================] - 14s 50ms/step - loss: 0.3590 - accuracy: 0.8785\n",
      "Epoch 15/50\n",
      "275/275 [==============================] - 15s 54ms/step - loss: 0.3481 - accuracy: 0.8832\n",
      "Epoch 16/50\n",
      "275/275 [==============================] - 17s 62ms/step - loss: 0.3066 - accuracy: 0.8964\n",
      "Epoch 17/50\n",
      "275/275 [==============================] - 19s 70ms/step - loss: 0.3086 - accuracy: 0.8954\n",
      "Epoch 18/50\n",
      "275/275 [==============================] - 21s 75ms/step - loss: 0.2961 - accuracy: 0.9012\n",
      "Epoch 19/50\n",
      "275/275 [==============================] - 14s 49ms/step - loss: 0.2732 - accuracy: 0.9089\n",
      "Epoch 20/50\n",
      "275/275 [==============================] - 12s 45ms/step - loss: 0.2409 - accuracy: 0.9191\n",
      "Epoch 21/50\n",
      "275/275 [==============================] - 13s 46ms/step - loss: 0.2472 - accuracy: 0.9168\n",
      "Epoch 22/50\n",
      "275/275 [==============================] - 12s 44ms/step - loss: 0.2409 - accuracy: 0.9180\n",
      "Epoch 23/50\n",
      "275/275 [==============================] - 12s 45ms/step - loss: 0.2326 - accuracy: 0.9207\n",
      "Epoch 24/50\n",
      "275/275 [==============================] - 15s 55ms/step - loss: 0.2592 - accuracy: 0.9164\n",
      "Epoch 25/50\n",
      "275/275 [==============================] - 14s 51ms/step - loss: 0.2114 - accuracy: 0.9289\n",
      "Epoch 26/50\n",
      "275/275 [==============================] - 14s 51ms/step - loss: 0.2094 - accuracy: 0.9312\n",
      "Epoch 27/50\n",
      "275/275 [==============================] - 14s 52ms/step - loss: 0.2081 - accuracy: 0.9318\n",
      "Epoch 28/50\n",
      "275/275 [==============================] - 13s 47ms/step - loss: 0.2113 - accuracy: 0.9300\n",
      "Epoch 29/50\n",
      "275/275 [==============================] - 12s 45ms/step - loss: 0.1863 - accuracy: 0.9368\n",
      "Epoch 30/50\n",
      "275/275 [==============================] - 13s 46ms/step - loss: 0.1896 - accuracy: 0.9376\n",
      "Epoch 31/50\n",
      "275/275 [==============================] - 13s 49ms/step - loss: 0.1871 - accuracy: 0.9384\n",
      "Epoch 32/50\n",
      "275/275 [==============================] - 14s 49ms/step - loss: 0.1730 - accuracy: 0.9416\n",
      "Epoch 33/50\n",
      "275/275 [==============================] - 13s 46ms/step - loss: 0.1770 - accuracy: 0.9415\n",
      "Epoch 34/50\n",
      "275/275 [==============================] - 13s 48ms/step - loss: 0.1640 - accuracy: 0.9454\n",
      "Epoch 35/50\n",
      "275/275 [==============================] - 13s 47ms/step - loss: 0.1669 - accuracy: 0.9447\n",
      "Epoch 36/50\n",
      "275/275 [==============================] - 15s 54ms/step - loss: 0.1632 - accuracy: 0.9453\n",
      "Epoch 37/50\n",
      "275/275 [==============================] - 17s 62ms/step - loss: 0.1643 - accuracy: 0.9459\n",
      "Epoch 38/50\n",
      "275/275 [==============================] - 21s 75ms/step - loss: 0.1700 - accuracy: 0.9445\n",
      "Epoch 39/50\n",
      "275/275 [==============================] - 16s 57ms/step - loss: 0.1572 - accuracy: 0.9469\n",
      "Epoch 40/50\n",
      "275/275 [==============================] - 17s 61ms/step - loss: 0.1463 - accuracy: 0.9522\n",
      "Epoch 41/50\n",
      "275/275 [==============================] - 17s 60ms/step - loss: 0.1402 - accuracy: 0.9528\n",
      "Epoch 42/50\n",
      "275/275 [==============================] - 17s 62ms/step - loss: 0.1939 - accuracy: 0.9394\n",
      "Epoch 43/50\n",
      "275/275 [==============================] - 17s 63ms/step - loss: 0.1516 - accuracy: 0.9506\n",
      "Epoch 44/50\n",
      "275/275 [==============================] - 17s 61ms/step - loss: 0.1418 - accuracy: 0.9515\n",
      "Epoch 45/50\n",
      "275/275 [==============================] - 16s 59ms/step - loss: 0.1474 - accuracy: 0.9501\n",
      "Epoch 46/50\n",
      "275/275 [==============================] - 18s 65ms/step - loss: 0.1378 - accuracy: 0.9544\n",
      "Epoch 47/50\n",
      "275/275 [==============================] - 21s 78ms/step - loss: 0.1428 - accuracy: 0.9536\n",
      "Epoch 48/50\n",
      "275/275 [==============================] - 17s 61ms/step - loss: 0.1427 - accuracy: 0.9545\n",
      "Epoch 49/50\n",
      "275/275 [==============================] - 23s 84ms/step - loss: 0.1397 - accuracy: 0.9533\n",
      "Epoch 50/50\n",
      "275/275 [==============================] - 19s 70ms/step - loss: 0.1330 - accuracy: 0.9571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25daa6b0970>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier.fit(x_train, y_train, epochs=50, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95e2e914-3c49-4d9c-b033-e72bed25581a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 3s 12ms/step - loss: 0.5164 - accuracy: 0.8275\n",
      "Accuracy:  0.8275237083435059\n"
     ]
    }
   ],
   "source": [
    "accuracy = classifier.evaluate(x = x_test, y = y_test, batch_size=32)\n",
    "print(\"Accuracy: \",accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2a936c58-2594-4dc6-a163-471b3ca75e33",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'drive' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m classifier\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCNNmodel.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m weights_file \u001b[38;5;241m=\u001b[39m \u001b[43mdrive\u001b[49m\u001b[38;5;241m.\u001b[39mCreateFile({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCNNmodel.h5\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m      3\u001b[0m weights_file\u001b[38;5;241m.\u001b[39mSetContentFile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCNNmodel.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m weights_file\u001b[38;5;241m.\u001b[39mUpload()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'drive' is not defined"
     ]
    }
   ],
   "source": [
    "classifier.save('CNNmodel.h5')\n",
    "weights_file = drive.CreateFile({'title' : 'CNNmodel.h5'})\n",
    "weights_file.SetContentFile('CNNmodel.h5')\n",
    "weights_file.Upload()\n",
    "drive.CreateFile({'id': weights_file.get('id')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "27df5cd8-a34e-495b-9712-a37b1e0fb5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from skimage.transform import resize, pyramid_reduce\n",
    "import PIL\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c671e0a4-8de4-47cb-abc8-f183ea76da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_predict(model, image):\n",
    "    data = np.asarray( image, dtype=\"int32\" )\n",
    "    pred_probab = model.predict(data)[0]\n",
    "    \n",
    "    # softmax gives probability for all the alphabets hence we have to choose the maximum probability alphabet \n",
    "    pred_class = list(pred_probab).index(max(pred_probab))\n",
    "    return max(pred_probab), pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0af630be-beb9-40cb-b24e-2c44819df5bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vid \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      2\u001b[0m img_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m      5\u001b[0m     \n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# ret, frame = vid.read()\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "vid = cv2.VideoCapture(0)\n",
    "img_counter = 0\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    # ret, frame = vid.read()\n",
    "    ret, frame = vid.read()\n",
    "    cv2.imshow('frame', frame)\n",
    "      \n",
    "    # the 'q' button : quitting button \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256 == 27:\n",
    "        # ESC pressed\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "    elif k%256 == 32:\n",
    "        # capturing the image from webcam \n",
    "        image_frame = vid.read()\n",
    "        \n",
    "        # # to crop required part\n",
    "        # im2 = crop_image(image_frame, 300,300,300,300)\n",
    "        im2 = image_frame\n",
    "\n",
    "        # convert to grayscale \n",
    "        image_grayscale = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # blurring the image \n",
    "        image_grayscale_blurred =cv2.GaussianBlur(image_grayscale, (15,15), 0)\n",
    "        \n",
    "        # resize the image to 28x28\n",
    "        im3 = cv2.resize(image_grayscale_blurred, (28,28), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "        # expand the dimensions from 28x28 to 1x28x28x1\n",
    "        im4 = np.resize(im3, (28, 28, 1))\n",
    "        im5 = np.expand_dims(im4, axis=0)\n",
    "        \n",
    "        pred_prob, pred_class = keras_predict(classifier, im5)\n",
    "        print(pred_class)\n",
    "        print(pred_prob)\n",
    "        time.sleep(15)\n",
    "        break\n",
    "\n",
    "# After the loop release the cap object\n",
    "vid.release()\n",
    "\n",
    "# Destroy all the windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f817a83-fcaa-45ca-b357-3c0999c33014",
   "metadata": {},
   "outputs": [],
   "source": [
    " # img_name = \"opencv_frame_{}.png\".format(0)\n",
    "        # cv2.imwrite(img_name, frame)\n",
    "        # print(\"{} written!\".format(img_name))\n",
    "        # img_counter += 1\n",
    "        \n",
    "#         time.sleep(10)\n",
    "\n",
    "        \n",
    "#         image = cv2.imread( \"opencv_frame_{}.png\".format(0))        \n",
    "#         # Select ROI\n",
    "#         # image = cv2.imread(\"11.jpg\")\n",
    "#         r = cv2.selectROI(\"select the area\", image)\n",
    "\n",
    "#         # Crop image\n",
    "#         cropped_image = image[int(r[1]):int(r[1]+r[3]),\n",
    "#                               int(r[0]):int(r[0]+r[2])]\n",
    "        \n",
    "#         img_name = \"opencv_frame_{}.png\".format(img_counter)\n",
    "#         cv2.imwrite(img_name, cropped_image)\n",
    "        \n",
    "#         # Display cropped image\n",
    "#         cv2.imshow(\"Cropped image\", cropped_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
